{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "from IES_Downloader import IES_Downloader\n",
    "import pandas as pd\n",
    "import math\n",
    "import operator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lxml in c:\\users\\marketa.cervena\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (4.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scrapy\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/e4/69b87d7827abf03dea2ea984230d50f347b00a7a3897bc93f6ec3dafa494/Scrapy-1.8.0-py2.py3-none-any.whl (238kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
      "Collecting queuelib>=1.4.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading https://files.pythonhosted.org/packages/db/6e/bf6d5e4d7cf233b785719aaec2c38f027b9c2ed980a0015ec1a1cced4893/Protego-0.1.16.tar.gz (3.2MB)\n",
      "Requirement already satisfied: pyOpenSSL>=16.2.0 in c:\\users\\marketa.cervena\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from scrapy) (19.1.0)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/45/1ba17c50a0bb16bd950c9c2b92ec60d40c8ebda9f3371ae4230c437120b6/w3lib-1.21.0-py2.py3-none-any.whl\n",
      "Collecting zope.interface>=4.1.3\n",
      "  Downloading https://files.pythonhosted.org/packages/94/81/8ba38f29bd5391563722b5c872701937b24dd0c70adbef87b7ce959dbc33/zope.interface-4.7.1-cp37-cp37m-win_amd64.whl (134kB)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: lxml>=3.5.0 in c:\\users\\marketa.cervena\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from scrapy) (4.4.1)\n",
      "Collecting Twisted>=17.9.0; python_version >= \"3.5\"\n",
      "  Downloading https://files.pythonhosted.org/packages/85/49/86fa99cc13bf1e4999141d997c00cd6ba07a86758449825547a5c2ac7c0a/Twisted-19.10.0-cp37-cp37m-win_amd64.whl (3.1MB)\n",
      "Collecting service-identity>=16.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e9/7c/2195b890023e098f9618d43ebc337d83c8b38d414326685339eb024db2f6/service_identity-18.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: cryptography>=2.0 in c:\\users\\marketa.cervena\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from scrapy) (2.8)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading https://files.pythonhosted.org/packages/86/c8/fc5a2f9376066905dfcca334da2a25842aedfda142c0424722e7c497798b/parsel-1.5.2-py2.py3-none-any.whl\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\marketa.cervena\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from scrapy) (1.13.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\marketa.cervena\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from zope.interface>=4.1.3->scrapy) (41.6.0.post20191030)\n",
      "Requirement already satisfied: attrs>=17.4.0 in c:\\users\\marketa.cervena\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from Twisted>=17.9.0; python_version >= \"3.5\"->scrapy) (19.3.0)\n",
      "Collecting Automat>=0.3.0\n",
      "  Downloading https://files.pythonhosted.org/packages/e5/11/756922e977bb296a79ccf38e8d45cafee446733157d59bcd751d3aee57f5/Automat-0.8.0-py2.py3-none-any.whl\n",
      "Collecting constantly>=15.1\n",
      "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
      "Collecting PyHamcrest>=1.9.0\n",
      "  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n",
      "Collecting incremental>=16.10.1\n",
      "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
      "Collecting hyperlink>=17.1.1\n",
      "  Downloading https://files.pythonhosted.org/packages/7f/91/e916ca10a2de1cb7101a9b24da546fb90ee14629e23160086cf3361c4fb8/hyperlink-19.0.0-py2.py3-none-any.whl\n",
      "Collecting pyasn1-modules\n",
      "  Downloading https://files.pythonhosted.org/packages/52/50/bb4cefca37da63a0c52218ba2cb1b1c36110d84dcbae8aa48cd67c5e95c2/pyasn1_modules-0.2.7-py2.py3-none-any.whl (131kB)\n",
      "Collecting pyasn1\n",
      "  Downloading https://files.pythonhosted.org/packages/62/1e/a94a8d635fa3ce4cfc7f506003548d0a2447ae76fd5ca53932970fe3053f/pyasn1-0.4.8-py2.py3-none-any.whl (77kB)\n",
      "Requirement already satisfied: cffi!=1.11.3,>=1.8 in c:\\users\\marketa.cervena\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from cryptography>=2.0->scrapy) (1.13.2)\n",
      "Requirement already satisfied: idna>=2.5 in c:\\users\\marketa.cervena\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from hyperlink>=17.1.1->Twisted>=17.9.0; python_version >= \"3.5\"->scrapy) (2.8)\n",
      "Requirement already satisfied: pycparser in c:\\users\\marketa.cervena\\appdata\\local\\continuum\\anaconda3\\envs\\py3\\lib\\site-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.19)\n",
      "Building wheels for collected packages: PyDispatcher, protego\n",
      "  Building wheel for PyDispatcher (setup.py): started\n",
      "  Building wheel for PyDispatcher (setup.py): finished with status 'done'\n",
      "  Created wheel for PyDispatcher: filename=PyDispatcher-2.0.5-cp37-none-any.whl size=12551 sha256=3c0fb2aaed7fef903e0e99ef4b015da27be1b1b26ec4e6facc11067811d42038\n",
      "  Stored in directory: C:\\Users\\marketa.cervena\\AppData\\Local\\pip\\Cache\\wheels\\88\\99\\96\\cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
      "  Building wheel for protego (setup.py): started\n",
      "  Building wheel for protego (setup.py): finished with status 'done'\n",
      "  Created wheel for protego: filename=Protego-0.1.16-cp37-none-any.whl size=7770 sha256=d65e1c051f09abea3953592b824be3eeb74fa4945ea18f1473ae281614416d6e\n",
      "  Stored in directory: C:\\Users\\marketa.cervena\\AppData\\Local\\pip\\Cache\\wheels\\51\\01\\d1\\4a2286a976dccd025ba679acacfe37320540df0f2283ecab12\n",
      "Successfully built PyDispatcher protego\n",
      "Installing collected packages: PyDispatcher, queuelib, protego, w3lib, zope.interface, cssselect, Automat, constantly, PyHamcrest, incremental, hyperlink, Twisted, pyasn1, pyasn1-modules, service-identity, parsel, scrapy\n",
      "Successfully installed Automat-0.8.0 PyDispatcher-2.0.5 PyHamcrest-1.9.0 Twisted-19.10.0 constantly-15.1.0 cssselect-1.1.0 hyperlink-19.0.0 incremental-17.5.0 parsel-1.5.2 protego-0.1.16 pyasn1-0.4.8 pyasn1-modules-0.2.7 queuelib-1.5.0 scrapy-1.8.0 service-identity-18.1.0 w3lib-1.21.0 zope.interface-4.7.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = (\"https://www.czc.cz/mobilni-telefony\",\"https://www.datart.cz/mobilni-telefony\",\"https://www.euronics.cz/mobilni-telefony\", \"https://www.mall.cz/https://www.mall.cz/mobilni-telefony\")\n",
    "\n",
    "shopnames = {\n",
    "  links[0]: \"CZC\",\n",
    "  links[1]: \"DATART\",\n",
    "  links[2]: \"EURONICS\",\n",
    " links[3]: \"MALL\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eshop:\n",
    "    '''\n",
    "   The main class containing all infomation gathered from eshop page, including the information that \n",
    "   requires web-specific scraping methods. As a \"bonus\", it also backs up the soup objects of all product pages in a SQL database\n",
    "    '''\n",
    "    def __init__(self, link, category = \"notebooky\"):\n",
    "        self.shop_name = link.split(\".\")[1].split(\"/\")[0]\n",
    "            \n",
    "        if self.shop_name == \"czc\": # if we scrape CZC eshop, use the link and categroy to create object \n",
    "            #from the specific CZC class that has its own scraping methods defined bellow\n",
    "            \n",
    "            self.shop_obj = CZC(link, category)\n",
    "            \n",
    "        elif self.shop_name == \"datart\": #similar for Datart eshop\n",
    "            \n",
    "            self.shop_obj == DATART(link, category)\n",
    "        \n",
    "        else:\n",
    "            print(\"Eshop {} out of scope\".format(self.shop_name))\n",
    "            self.shop_obj == []\n",
    "            \n",
    "        try: \n",
    "            self.robots_link = link + \"/robots.txt\"\n",
    "            self.robots = requests.get(self.robots_link).text\n",
    "        except: \n",
    "            print(\"Cannot reach robots.txt sie\")\n",
    "            \n",
    "        self.cat_link = [] #link to to the first product page within category\n",
    "        self.itms = [] #number of items in category\n",
    "        self.pgs = [] # number of product pages in cateogory\n",
    "        self.itmppg = [] #items per page\n",
    "        self.urls = [] #list of URLs to all product pages within category\n",
    "        self.allsoups = [] #list of BS objects for the urls list\n",
    "        self.all_pages = [] #list of page objects from the BSs\n",
    "        self.prd_modules = [] #list of product modules found in the product page BSs\n",
    "        self.all_prds = []#list of product objects\n",
    "        self.prd_data = {\n",
    "                'product':[],\n",
    "                'price':[]} \n",
    "        \n",
    "\n",
    "    def get_catlink(self):\n",
    "        '''\n",
    "        formulation of the main product page link requires eshop specific method -\n",
    "        '''    \n",
    "        self.shop_obj.get_catlink()\n",
    "        self.cat_link = self.shop_obj.cat_link\n",
    "        \n",
    "    \n",
    "    def get_homesoup(self):#generic method\n",
    "        '''\n",
    "        simplegeneric  request method \n",
    "        '''     \n",
    "        #try: \n",
    "        r = requests.get(self.cat_link)\n",
    "        r.encoding='UTF-8'\n",
    "        self.home_soup = BeautifulSoup(r.text,'lxml')\n",
    "            \n",
    "        #some expect here \"Could not request the eshop main page\"\n",
    "        \n",
    "    def leverage_soup(self): \n",
    "        '''\n",
    "        gathering initial information from the first product page via self.shop_obj \n",
    "        '''  \n",
    "        self.shop_obj.get_homesoupinfo(self.home_soup)\n",
    "        self.pgs = self.shop_obj.pgs\n",
    "        self.itms = self.shop_obj.itms\n",
    "        self.itmppg = math.ceil(self.itms / self.pgs)\n",
    "             \n",
    "        self.urls = [None]*self.pgs\n",
    "        \n",
    "        self.urls[0] = self.cat_link\n",
    "        \n",
    "        for i in range(1,self.pgs):\n",
    "            self.urls[i] = self.cat_link + self.shop_obj.urlspec + str(self.itmppg*i)\n",
    "    \n",
    "    def get_allsoups(self):\n",
    "        '''\n",
    "        lists soup objects of all product pages within the category\n",
    "        '''  \n",
    "        self.allsoups = [None]*self.pgs\n",
    "        mm = \"\"\n",
    "        \n",
    "        for i in range(0,self.pgs):\n",
    "            #try:\n",
    "            r = requests.get(self.urls[i])\n",
    "            r.encoding='UTF-8'\n",
    "            self.allsoups[i] = BeautifulSoup(r.text,'lxml')\n",
    "                #some exceot \n",
    "                #m = \"Could not request {}th product page from {} eshop\".format(i,self.shop_name)\n",
    "                #mm = appent(mm,m)\n",
    "                #print(mm)\n",
    "            \n",
    "            \n",
    "    def get_pages(self):\n",
    "        '''\n",
    "        creates eshop-specific list of page objects \n",
    "        '''  \n",
    "        self.pages = self.shop_obj.get_pages(self.allsoups)\n",
    "        \n",
    "        for i in range (0,self.pgs):\n",
    "            self.prd_modules[((0+i)*self.itmppg):((1+i)*self.itmppg)] = self.pages[i].prodmod\n",
    "        \n",
    "        \n",
    "    def get_products(self):\n",
    "        '''\n",
    "        Extrect the relevant information from the identified product modulesusingehop-specific methods\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        for i in range (0,self.itms):\n",
    "            self.all_prds[i] = product(self.shop_object, self.prd_modules[i]).GetTitle().GetPrice()\n",
    "            self.prd_data['product'][i] = self.all_prds[i].title\n",
    "            self.prd_data['price'][i] = self.all_prds[i].price\n",
    "            \n",
    "        \n",
    "    def get_output(self):\n",
    "        '''\n",
    "       Formulate the collected information in a panda frame\n",
    "        '''\n",
    "        \n",
    "\n",
    "    def update(self):\n",
    "        '''\n",
    "       Update database\n",
    "        '''\n",
    "\n",
    "\n",
    "    \n",
    "              \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DATART:\n",
    "    '''\n",
    "    Specific clas for DATART eshop containing its web specific scraping methods under alligned terminology in\n",
    "    order to be callable in the eshop class \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, link, category):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        self.link = link\n",
    "        self.category = category\n",
    "        self.cat_link = []\n",
    "        self.itms = []\n",
    "        self.pgs = []\n",
    "        self.urlspec = \"/?startPos=\"\n",
    "        self.pages = []\n",
    "        self.products = []\n",
    "        \n",
    " \n",
    "        \n",
    "        \n",
    "    def get_catlink(self):\n",
    "        '''\n",
    "        Contruct the actual link for relevant category in DATART eshop  \n",
    "        '''    \n",
    "        self.cat_link = \"{}/{}\".format(self.link, self.category)\n",
    "        \n",
    "    def get_homesoupinfo(self, soup):\n",
    "        '''\n",
    "        Download the beautiful soup object from the first product page in selected category on DATART and extract available information\n",
    "        '''    \n",
    "        self.itms = int(soup.findAll(\"div\",{'class','pagination-show-page'})[0].findAll('strong')[1].text)\n",
    "        self.pgs =  int(soup.findAll(\"a\",{'class','button filter-attr'})[2].text)\n",
    "        \n",
    "    def get_pages(self, souplist):\n",
    "        \n",
    "        self.pages = [None]*self.pgs\n",
    "        for i in range(0,len(souplist)):\n",
    "            self.pages[i] = page(souplist[i]).get_prodsoups()\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "         \n",
    "    \n",
    "class page(DATART):\n",
    "        \n",
    "    def __init__(self, soup):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        self.soup = soup\n",
    "        self.prodmod = []\n",
    "        self.products = []\n",
    "    \n",
    "    def get_prodsoups(self):\n",
    "        '''\n",
    "        Extracts the product modules pro soup of all pages within the category\n",
    "        '''\n",
    "        self.prodmod = self.soup.findAll('div', {'class':'category-page-item grid fly-parent wa-product-impression'})\n",
    "        get_products(self.prodmod)\n",
    "        \n",
    "    def get_products(self, souplist):\n",
    "        \n",
    "        self.products = [None]*self.itms\n",
    "        for i in range(0,len(souplist)):\n",
    "            self.products[i] = product(souplist[i]).get_title().get_price()\n",
    "    \n",
    "class product(DATART):\n",
    "    \n",
    "    def __init__(self, soup):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        self.soup = soup\n",
    "        self.title = []\n",
    "        self.price = []\n",
    "        self.currency = \"CZK\"\n",
    "    \n",
    "    \n",
    "    def get_title(self):\n",
    "        '''\n",
    "        A method to extract product name from product module soup \n",
    "        '''\n",
    "        self.title = this_soup.find('h3',{'class':'item-title'}).text.replace(u'\\r', u'').replace(u'\\n', u'').split(\" (\")[0]\n",
    "     \n",
    "    def get_price(self): \n",
    "        '''\n",
    "        A method to extract product price from product module soup \n",
    "        '''\n",
    "        self.price =  int(this_soup.find('span',{'class':'actual'}).text.split(\":\")[1].replace(u'\\xa0', u'').split(\"KÄ\")[0])\n",
    "           \n",
    "    \n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "         \n",
    "        \n",
    "        \n",
    "class CZC:\n",
    "    '''\n",
    "    Specific clas for CZC eshop containing its web specific scraping methods under alligned terminology in\n",
    "    order to be callable in the eshop class \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, link, category):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        \n",
    "        self.link = link\n",
    "        self.category = category\n",
    "        self.cat_link = []\n",
    "        self.itms = []\n",
    "        self.pgs = []\n",
    "        self.urlspec = \"?q-first=\"\n",
    "        self.pages = []\n",
    "        self.products = []\n",
    "            \n",
    "        \n",
    "    def get_catlink(self):\n",
    "        '''\n",
    "        Contruct the actual link for relevant category in CZC eshop  \n",
    "        '''\n",
    "        self.cat_link = \"{}/{}/produkty\".format(self.link, self.category)\n",
    "    \n",
    "    def get_homesoupinfo(self,soup):\n",
    "        '''\n",
    "        Download the beautiful soup object from the first product page in selected category on CZC and extract available information\n",
    "        '''    \n",
    "        self.itms = int(str(soup.findAll('div',{'class','order-by-sum h-800'})).split(\">\")[1].split(\"<\")[0].split(\" \")[0].replace(u'\\xa0', u''))\n",
    "        self.pgs = math.ceil(self.itms/27)\n",
    "        \n",
    "    def get_pages(self, souplist):\n",
    "        \n",
    "        self.pages = [None]*self.pgs\n",
    "        for i in range(0,len(souplist)):\n",
    "            self.pages[i] = page(souplist[i]).get_prodsoups() \n",
    "            \n",
    "class page(CZC):\n",
    "        \n",
    "    def __init__(self, soup):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        self.soup = soup\n",
    "        self.prodmod = []\n",
    "        self.products = []\n",
    "    \n",
    "    def get_prodsoups(self):\n",
    "        '''\n",
    "        Extracts the product modules pro soup of all pages within the category\n",
    "        '''\n",
    "        self.prodmod = self.soup.findAll('div', {'class':'overflow'})\n",
    "        get_products(self.prodmod)\n",
    "        \n",
    "    def get_products(self, souplist):\n",
    "        \n",
    "        self.products = [None]*self.itms\n",
    "        for i in range(0,len(souplist)):\n",
    "            self.products[i] = product(souplist[i]).get_title().get_price()\n",
    "        \n",
    "class product(CZC):\n",
    "        \n",
    "    def __init__(self, soup):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        self.soup = soup\n",
    "        self.title = []\n",
    "        self.price = []\n",
    "        self.currency = \"CZK\"\n",
    "    \n",
    "    \n",
    "    def GetTitle(self):\n",
    "        '''\n",
    "        A method to extract product name from product module soup \n",
    "        '''\n",
    "        self.title = self.soup.find('a')['title'].split(\",\")[0].split(\" +\")[0]\n",
    "     \n",
    "    def GetPrice(self): \n",
    "        '''\n",
    "        A method to extract product price from product module soup \n",
    "        '''\n",
    "        price =self.soup.findAll('span',{'class':'price-vatin'})\n",
    "        l=len(price)-1\n",
    "        self.price = int(price[l].text.replace(u'\\xa0', u'').split(\"KÄ\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = eshop(\"https://www.czc.cz\",\"notebooky\")\n",
    "R.get_catlink()\n",
    "#R.get_homesoup()\n",
    "#R.leverage_soup()\n",
    "#R.get_allsoups()\n",
    "#R.get_pages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'czc'"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R.shop_name\n",
    "R.cat_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
