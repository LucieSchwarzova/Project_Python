{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# from tqdm import tqdm\n",
    "# import pandas as pd\n",
    "# import math\n",
    "# import operator\n",
    "# from datetime import datetime\n",
    "# import sqlite3\n",
    "# import random\n",
    "# import os\n",
    "# import time\n",
    "# import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DATART:\n",
    "    '''\n",
    "    Specific class for DATART eshop containing its web specific scraping methods under alligned terminology in\n",
    "    order to be callable in the eshop class \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, link, category):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        self.link = link\n",
    "        self.category = category\n",
    "        self.cat_link = []\n",
    "        self.itms = []\n",
    "        self.pgs = []\n",
    "        self.urlspec = \"/?startPos=\"\n",
    "        self.pages = []\n",
    "        self.products = []\n",
    "               \n",
    "        \n",
    "    def get_catlink(self):\n",
    "        '''\n",
    "        Contruct the actual link for relevant category in DATART eshop  \n",
    "        '''    \n",
    "        self.cat_link = \"{}/{}\".format(self.link, self.category)\n",
    "        \n",
    "    def get_homesoupinfo(self, soup):\n",
    "        '''\n",
    "        Download the beautiful soup object from the first product page in selected category on DATART and extract available information\n",
    "        '''    \n",
    "        self.itms = int(soup.findAll(\"div\",{'class','pagination-show-page'})[0].findAll('strong')[1].text)\n",
    "        \n",
    "        try: \n",
    "            self.pgs  = int(soup.findAll(\"a\",{'class','button filter-attr'})[2].text)/1\n",
    "        except:\n",
    "            self.pgs = 1 \n",
    "        \n",
    "    def get_pages(self, souplist):\n",
    "        \n",
    "        self.pages = [None]*self.pgs\n",
    "        for i in range(0,len(souplist)):\n",
    "            self.pages[i] = page(souplist[i])\n",
    "            \n",
    "            \n",
    "    def get_products(self, souplist):\n",
    "        \n",
    "        self.products = [None]*self.itms\n",
    "        for i in range(0,len(souplist)):\n",
    "            self.products[i] = product(souplist[i])\n",
    "    \n",
    "class page(DATART):\n",
    "        \n",
    "    def __init__(self, soup):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        self.soup = soup\n",
    "        self.prodmod = self.soup.findAll('div', {'class':'category-page-item grid fly-parent wa-product-impression'})\n",
    "             \n",
    "    \n",
    "class product(DATART):\n",
    "    \n",
    "    def __init__(self, soup):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        self.soup = soup\n",
    "        self.title = []\n",
    "        self.price = []\n",
    "        self.currency = \"CZK\"\n",
    "        self.get_title()\n",
    "        self.get_price()\n",
    "    \n",
    "    \n",
    "    def get_title(self):\n",
    "        '''\n",
    "        A method to extract product name from product module soup \n",
    "        '''\n",
    "        self.title = self.soup.find('h3',{'class':'item-title'}).text.replace(u'\\r', u'').replace(u'\\n', u'').split(\" (\")[0]\n",
    "     \n",
    "    def get_price(self): \n",
    "        '''\n",
    "        A method to extract product price from product module soup \n",
    "        '''\n",
    "        self.price =  int(self.soup.find('span',{'class':'actual'}).text.split(\":\")[1].replace(u'\\xa0', u'').split(\"Kč\")[0])\n",
    "           \n",
    "    \n",
    "\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "       \n",
    "class CZC:\n",
    "    '''\n",
    "    Specific clas for CZC eshop containing its web specific scraping methods under alligned terminology in\n",
    "    order to be callable in the eshop class \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, link, category):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        \n",
    "        self.link = link\n",
    "        self.category = category\n",
    "        self.cat_link = []\n",
    "        self.itms = []\n",
    "        self.pgs = []\n",
    "        self.urlspec = \"?q-first=\"\n",
    "        self.pages = []\n",
    "        self.products = []\n",
    "            \n",
    "        \n",
    "    def get_catlink(self):\n",
    "        '''\n",
    "        Contruct the actual link for relevant category in CZC eshop  \n",
    "        '''\n",
    "        self.cat_link = \"{}/{}/produkty\".format(self.link, self.category)\n",
    "    \n",
    "    def get_homesoupinfo(self,soup):\n",
    "        '''\n",
    "        Download the beautiful soup object from the first product page in selected category on CZC and extract available information\n",
    "        '''    \n",
    "        self.itms = int(str(soup.findAll('div',{'class','order-by-sum h-800'})).split(\">\")[1].split(\"<\")[0].split(\" \")[0].replace(u'\\xa0', u''))\n",
    "        self.pgs = math.ceil(self.itms/27)\n",
    "        \n",
    "    def get_pages(self, souplist):\n",
    "        \n",
    "        self.pages = [None]*self.pgs\n",
    "        for i in range(0,len(souplist)):\n",
    "            self.pages[i] = page(souplist[i])\n",
    "    \n",
    "        \n",
    "    def get_products(self, souplist):\n",
    "        \n",
    "        self.products = [None]*self.itms\n",
    "        for i in range(0,len(souplist)):\n",
    "            self.products[i] = product(souplist[i])\n",
    "            \n",
    "class page(CZC):\n",
    "        \n",
    "    def __init__(self, soup):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        self.soup = soup\n",
    "        self.prodmod = self.soup.findAll('div', {'class':'overflow'})\n",
    "\n",
    "        \n",
    "class product(CZC):\n",
    "        \n",
    "    def __init__(self, soup):\n",
    "        '''\n",
    "        Define the empty generic eshop structure including the items that require web specific scraping  \n",
    "        '''\n",
    "        self.soup = soup\n",
    "        self.title = []\n",
    "        self.price = []\n",
    "        self.currency = \"CZK\"\n",
    "        self.get_title()\n",
    "        self.get_price()\n",
    "    \n",
    "    \n",
    "    def get_title(self):\n",
    "        '''\n",
    "        A method to extract product name from product module soup \n",
    "        '''\n",
    "        self.title = self.soup.find('a')['title'].split(\",\")[0].split(\" +\")[0]\n",
    "     \n",
    "    def get_price(self): \n",
    "        '''\n",
    "        A method to extract product price from product module soup \n",
    "        '''\n",
    "        price =self.soup.findAll('span',{'class':'price-vatin'})\n",
    "        l=len(price)-1\n",
    "        self.price = int(price[l].text.replace(u'\\xa0', u'').split(\"Kč\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class eshop:\n",
    "    '''\n",
    "    The main class containing all infomation gathered from eshop page, including the information that \n",
    "    requires web-specific scraping methods. As a \"bonus\", it also backs up the soup objects of all product pages in a SQL database\n",
    "    '''\n",
    "    def __init__(self, link, category = \"notebooky\"):\n",
    "       \n",
    "        self.shop_name = link.split(\".\")[1].split(\"/\")[0]\n",
    "            \n",
    "        if self.shop_name == \"czc\": # if we scrape CZC eshop, use the link and categroy to create object \n",
    "            #from the specific CZC class that has its own scraping methods defined bellow\n",
    "            \n",
    "            self.shop_obj = CZC(link, category)\n",
    "            \n",
    "        elif self.shop_name == \"datart\": #similar for Datart eshop\n",
    "            \n",
    "            self.shop_obj = DATART(link, category)\n",
    "        \n",
    "        else:\n",
    "            print(\"Eshop {} out of scope\".format(self.shop_name))\n",
    "            self.shop_obj == []\n",
    "            \n",
    "        try: \n",
    "            self.robots_link = link + \"/robots.txt\"\n",
    "            self.robots = requests.get(self.robots_link).text\n",
    "        except: \n",
    "            print(\"Cannot reach robots.txt\")\n",
    "            \n",
    "        self.cat_link = [] #link to to the first product page within category\n",
    "        self.itms = [] #number of items in category\n",
    "        self.pgs = [] # number of product pages in cateogory\n",
    "        self.itmppg = [] #items per page\n",
    "        self.urls = [] #list of URLs to all product pages within category\n",
    "        self.allsoups = [] #list of BS objects for the urls list\n",
    "        self.all_pages = [] #list of page objects from the BSs\n",
    "        self.prd_modules = [] #list of product modules found in the product page BSs\n",
    "        self.all_prds = []#list of product objects\n",
    "        self.prd_data = {\n",
    "                'product':[],\n",
    "                'price':[]} \n",
    "        self.timestamp = []\n",
    "        self.get_catlink()\n",
    "        self.get_homesoup()\n",
    "        self.leverage_soup()\n",
    "        self.get_allsoups()\n",
    "        self.get_pages()\n",
    "        self.get_products()\n",
    "        self.timestamp = datetime.now()\n",
    "        self.output = pd.DataFrame(self.prd_data)\n",
    "  \n",
    "\n",
    "    def get_catlink(self):\n",
    "        '''\n",
    "        formulation of the main product page link requires eshop specific method -\n",
    "        '''    \n",
    "        self.shop_obj.get_catlink()\n",
    "        self.cat_link = self.shop_obj.cat_link\n",
    "        \n",
    "    \n",
    "    def get_homesoup(self):#generic method\n",
    "        '''\n",
    "        simplegeneric  request method \n",
    "        '''     \n",
    "        try: \n",
    "            r = requests.get(self.cat_link)\n",
    "            r.encoding='UTF-8'\n",
    "            self.home_soup = BeautifulSoup(r.text,'lxml')\n",
    "            print(\"Successfully requested the first {} homepage soup\".format(self.shop_name))   \n",
    "        \n",
    "        except:\n",
    "            print(\"Could not request the eshop main page or {} eshop\".format(self.shop_name))\n",
    "        \n",
    "    def leverage_soup(self): \n",
    "        '''\n",
    "        gathering initial information from the first product page via self.shop_obj \n",
    "        '''  \n",
    "        try:\n",
    "            self.shop_obj.get_homesoupinfo(self.home_soup)\n",
    "        except:\n",
    "            print(\"Could not run the e-shop specific information gathering method \\n\")\n",
    "        self.pgs = self.shop_obj.pgs\n",
    "        print(\"{} pages found\\n\".format(self.pgs))\n",
    "        self.itms = self.shop_obj.itms\n",
    "        print(\"{} items found\\n\".format(self.itms))\n",
    "\n",
    "        self.itmppg = math.ceil(self.itms / self.pgs)\n",
    "             \n",
    "        self.urls = [None]*self.pgs\n",
    "        \n",
    "        self.urls[0] = self.cat_link\n",
    "        \n",
    "        for i in range(1,self.pgs):\n",
    "            self.urls[i] = self.cat_link + self.shop_obj.urlspec + str(self.itmppg*i)\n",
    "        \n",
    "        print(\"Product page urls were generated\")\n",
    "    \n",
    "    def get_allsoups(self):\n",
    "        '''\n",
    "        lists soup objects of all product pages within the category\n",
    "        '''  \n",
    "        self.allsoups = [None]*self.pgs\n",
    "        \n",
    "        for i in range(0,self.pgs):\n",
    "            try:\n",
    "                r = requests.get(self.urls[i])\n",
    "                r.encoding='UTF-8'\n",
    "                self.allsoups[i] = BeautifulSoup(r.text,'lxml')\n",
    "                print( \"{}th product page from {} eshop successfully requested \\n\".format(i +1,self.shop_name))\n",
    " \n",
    "            except:\n",
    "                print( \"Could not request {}th product page from {} eshop \\n\".format(i+1,self.shop_name))\n",
    "            \n",
    "            time.sleep(random.randrange(0,300,1)/100) # in order not to be caught by web's robotic behavior deection systems\n",
    "        \n",
    "            \n",
    "            \n",
    "    def get_pages(self):\n",
    "        '''\n",
    "        creates eshop-specific list of page objects \n",
    "        '''\n",
    "        self.shop_obj.get_pages(self.allsoups)\n",
    "        self.pages = self.shop_obj.pages\n",
    "        \n",
    "        for i in range (0,self.pgs):\n",
    "            self.prd_modules[((0+i)*self.itmppg):((1+i)*self.itmppg)] = self.pages[i].prodmod\n",
    "        \n",
    "        \n",
    "    def get_products(self):\n",
    "        '''\n",
    "        Extrect the relevant information from the identified product modulesusingehop-specific methods\n",
    "        '''\n",
    "        self.shop_obj.get_products(self.prd_modules)\n",
    "        self.all_prds = self.shop_obj.products\n",
    "        self.prd_data['product'] = list(o.title for o in self.all_prds)\n",
    "        self.prd_data['price'] = list(o.price for o in self.all_prds)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class collect:\n",
    "    '''\n",
    "    This class contains method to collect the data from selected category and eshop. Some execution \n",
    "    parameters (period, iterations) can be also provided, while their default value is one, i.e. \n",
    "    one execution only, data is collected,not updated further. \n",
    "    '''\n",
    "    \n",
    "    def __init__(self, link, category,period=1, iterations=1):\n",
    "        print('You are about to start scraping the {} category. The page will be re-scanned every {} seconds, in total {} times'.format(category,period,iterations))\n",
    "        self.timestamp = int(time.time())\n",
    "        self.link = link\n",
    "        self.category = category\n",
    "        self.soup_tablename = 'soup'+str(self.timestamp)\n",
    "        self.product_tablename  = 'product'+str(self.timestamp)\n",
    "        self.request = []\n",
    "        self.get_storage_ready()\n",
    "        self.iterator(link, category, period,iterations)\n",
    "        \n",
    "        \n",
    "                                            \n",
    "    def get_storage_ready(self):                                   \n",
    "       \n",
    "        data_storage = sqlite3.connect('storage.db').cursor()\n",
    "        data_storage.execute(\"\"\"CREATE TABLE IF NOT EXISTS {}(\n",
    "                                                            shop TEXT,\n",
    "                                                            category TEXT,\n",
    "                                                            soup LONGTEXT,\n",
    "                                                            stamp INTEGER)\"\"\".format(self.soup_tablename))\n",
    "        \n",
    "        data_storage.execute(\"\"\"CREATE TABLE IF NOT EXISTS {}(\n",
    "                                                            shop TEXT,\n",
    "                                                            category TEXT,\n",
    "                                                            product TEXT,\n",
    "                                                            price INTEGER,\n",
    "                                                            stamp INTEGER)\"\"\".format(self.product_tablename)) \n",
    "        \n",
    "        data_storage.close()\n",
    "        \n",
    "        print(\"Current table for product data: {}\".format(self.product_tablename)) \n",
    "        print(\"Current table for soups: {}\".format(self.soup_tablename))                               \n",
    "        print(\"Storage is ready. Data collection is about to start. The scraped data will be stored in *storage.db* database in your woking directory /n\")\n",
    "      \n",
    "    \n",
    "        \n",
    "           \n",
    "    def initiate_request(self):\n",
    "        \"\"\"\n",
    "        Request the link / category and scape all the requested information\n",
    "        \"\"\"\n",
    "        self.request = eshop(self.link,self.category)\n",
    "           \n",
    "    \n",
    "    def iterator(self, link,category,period,iterations):\n",
    "        \n",
    "        self.initiate_request()\n",
    "        self.data_upload()\n",
    "        \n",
    "        for i in range(iterations-1):\n",
    "            print(\"{} iteration out of {} commited, next to be starter in {} mins \\n\".format(i+1,iterations, period/60))\n",
    "            time.sleep(period) \n",
    "            self.initiate_request()\n",
    "            self.timestamp = int(time.time())\n",
    "            self.data_upload()\n",
    "            print(\"{} iteration out of {} commited, next to be starter in {} mins \\n\".format(i,iterations, period/60))\n",
    "               \n",
    "    def data_upload(self):\n",
    "        print(\"Data upload just started\\n\")\n",
    "        try:\n",
    "            data_storage = sqlite3.connect('storage.db')\n",
    "            table = self.product_tablename\n",
    "            link = self.link\n",
    "            category = self.category\n",
    "            time = self.timestamp\n",
    "            \n",
    "            for i in range(self.request.itms):\n",
    "                price = self.request.output['price'][i]\n",
    "                product = self.request.output['product'][i]\n",
    "                data_storage = sqlite3.connect('storage.db')\n",
    "                data_storage.execute( \"\"\"INSERT INTO {}(shop, category, product, price, stamp)\n",
    "                VALUES ('{}','{}','{}','{}','{}')\"\"\".format(table, link, category, product, price, time))\n",
    "                data_storage.execute(\"\"\"COMMIT\"\"\")\n",
    "                print(\"Recod {}/{} was added in the database\\n\".format(i+1,self.request.itms))\n",
    "                data_storage.close()\n",
    "            print(\"All changes successfully commited \\n\")\n",
    "            \n",
    "        except:\n",
    "            print(\"An error occured in data upload\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are about to start scraping the jednokolky category. The page will be re-scanned every 10 seconds, in total 3 times\n",
      "Current table for product data: product1579364269\n",
      "Current table for soups: soup1579364269\n",
      "Storage is ready. Data collection is about to start. The scraped data will be stored in *storage.db* database in your woking directory /n\n",
      "Successfully requested the first datart homepage soup\n",
      "1 pages found\n",
      "\n",
      "3 items found\n",
      "\n",
      "Product page urls were generated\n",
      "1th product page from datart eshop successfully requested \n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'title'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-910eadad6ad0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"https://www.datart.cz/\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"jednokolky\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-55-f0d325212803>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, link, category, period, iterations)\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_storage_ready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-f0d325212803>\u001b[0m in \u001b[0;36miterator\u001b[1;34m(self, link, category, period, iterations)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mperiod\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0miterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 55\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minitiate_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     56\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_upload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-f0d325212803>\u001b[0m in \u001b[0;36minitiate_request\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mRequest\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mcategory\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mscape\u001b[0m \u001b[0mall\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mrequested\u001b[0m \u001b[0minformation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \"\"\"\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meshop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlink\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-4a30f0aece81>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, link, category)\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_allsoups\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_pages\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_products\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprd_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-4a30f0aece81>\u001b[0m in \u001b[0;36mget_products\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshop_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_products\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprd_modules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_prds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshop_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproducts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprd_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_prds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprd_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprice\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_prds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-54-4a30f0aece81>\u001b[0m in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshop_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_products\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprd_modules\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_prds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshop_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mproducts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprd_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'product'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_prds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprd_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'price'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprice\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_prds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'title'"
     ]
    }
   ],
   "source": [
    "collect(\"https://www.datart.cz/\",\"jednokolky\",10,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
